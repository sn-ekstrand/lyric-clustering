{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import itertools\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist, squareform \n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/lyrics_cleaned.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's check out a random song\n",
    "idx = np.random.randint(0,6252)\n",
    "print(df.iloc[idx, 0], '-', df.iloc[idx, 1], '\\n', \n",
    "      '--------------',\n",
    "      '\\n', df.iloc[idx, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X = vectorizer.fit_transform(df['lyrics'])\n",
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6252, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idfs_dict = {}\n",
    "for word, idf in zip(vectorizer.vocabulary_, vectorizer.idf_):\n",
    "    word_idfs_dict[word] = idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_7 = {}\n",
    "ls_5 = {}\n",
    "idfs_5_7 = {}\n",
    "for k, v in word_idfs_dict.items():\n",
    "    if v > 7:\n",
    "        gt_7[k] = v\n",
    "    if v < 5:\n",
    "        ls_5[k] = v\n",
    "    else:\n",
    "        idfs_5_7[k] = v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf greater than 7: sometimes informative but usually not\n",
    "gt_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf less than 5: again, sometimes informative but usually not\n",
    "ls_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs_5_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans = KMeans(n_clusters=6)\n",
    "# kmeans.fit(X)\n",
    "\n",
    "# top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "# print(\"\\n3) top features (words) for each cluster:\")\n",
    "# for num, centroid in enumerate(top_centroids):\n",
    "#     print(\"%d: %s\" % (num, \", \".join(features[i] for i in centroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not really useful outputs. Contractions are being split and shouldn't be. Too many words are common across all found topics. Need to adjust the stopwords a bit. Might find some value in increasing n-grams.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features=10000)\n",
    "X = vectorizer.fit_transform(df['lyrics'])\n",
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idxs = np.argsort(vectorizer.idf_)[::-1]\n",
    "bottom_idxs = np.argsort(vectorizer.idf_)[::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_100_words = [features[i] for i in bottom_idxs[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bottom_100_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "lyric_stopwords = set(stopwords.words('english') + bottom_100_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=list(lyric_stopwords), \n",
    "                             max_features=5000, \n",
    "                             ngram_range=(2,5))\n",
    "X = vectorizer.fit_transform(df['lyrics'])\n",
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nmf(r, data):\n",
    "    nmf = NMF(n_components=r, init='random', random_state=0)\n",
    "    nmf.fit(data)\n",
    "    W = nmf.transform(data)\n",
    "    H = nmf.components_\n",
    "    return nmf.reconstruction_err_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_size = range(1, 10)\n",
    "error = [fit_nmf(i, X) for i in range_size]\n",
    "plt.plot(range_size, error)\n",
    "plt.xticks(range_size)\n",
    "plt.xlabel('r')\n",
    "plt.ylabel('Reconstruction Errror')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting to see an elbow at n_components=5 with n_grams ranging from 2-10. But the reconstruction error is still really high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try just looking at few artists at first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('artist').count().sort_values(by='title', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eminem_chesney = df[(df['artist'] == 'Eminem') | (df['artist'] == 'Kenny Chesney')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eminem_chesney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features=5000, \n",
    "                             ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(eminem_chesney['lyrics'])\n",
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_size = range(1, 10)\n",
    "error = [fit_nmf(i, X) for i in range_size]\n",
    "plt.plot(range_size, error)\n",
    "plt.xticks(range_size)\n",
    "plt.xlabel('r')\n",
    "plt.ylabel('Reconstruction Errror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reconstruction_err_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_label_topics(H, vocabulary):\n",
    "    '''\n",
    "    Print the most influential words of each latent topic, and prompt the user\n",
    "    to label each topic. The user should use their humanness to figure out what\n",
    "    each latent topic is capturing.\n",
    "    '''\n",
    "    hand_labels = []\n",
    "    for i, row in enumerate(H):\n",
    "        top_five = np.argsort(row)[::-1][:10]\n",
    "        print('topic', i)\n",
    "        print('-->', ' '.join(vocabulary[top_five]))\n",
    "#         label = input('please label this topic: ')\n",
    "#         hand_labels.append(label)\n",
    "#         print()\n",
    "    return hand_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "hand_label_topics(H, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[(df['artist'] == 'Eminem')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df['lyrics'][3753])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[(df['artist'] == 'Kenny Chesney')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['lyrics'][4033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drake and Rascal Flats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drake_rascal = df[(df['artist'] == 'Drake') | (df['artist'] == 'Rascal Flats')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features=5000, \n",
    "                             ngram_range=(1,6))\n",
    "X = vectorizer.fit_transform(drake_rascal['lyrics'])\n",
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_size = range(1, 10)\n",
    "error = [fit_nmf(i, X) for i in range_size]\n",
    "plt.plot(range_size, error)\n",
    "plt.xticks(range_size)\n",
    "plt.xlabel('r')\n",
    "plt.ylabel('Reconstruction Errror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=2, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "hand_label_topics(H, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing gears a bit. Let's see if spaCy gives us more control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eminem = df[df['artist'] == 'Eminem']['lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eminem[1585]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for doc in eminem:\n",
    "    docs.append(nlp(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "song_tokens = []\n",
    "for doc in docs:\n",
    "    doc_lemmas = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ in spacy_stopwords:\n",
    "            continue\n",
    "        elif token.lemma_ in [' ', ',', ']', '[', ':', '  ', '-']:\n",
    "            continue\n",
    "        else:\n",
    "            doc_lemmas.append(token.lemma_)\n",
    "    song_tokens.append(doc_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_tokens[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for song in song_tokens:\n",
    "word_vecs = []\n",
    "for token in song_tokens[9]:\n",
    "    word_vecs.append(nlp(token).vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vector = np.zeros((96,))\n",
    "for word in word_vecs:\n",
    "    mean_vector += word\n",
    "mean_vector /= len(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a single vector for a song\n",
    "# \n",
    "mean_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
